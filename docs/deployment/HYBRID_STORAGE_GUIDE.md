# BuffetBot Hybrid Storage Implementation Guide

## ðŸŽ¯ Overview

This guide implements the ultra-low-cost storage strategy combining:
- **Serverless NoSQL** for real-time operational data
- **Object Storage** for historical data archival
- **Analytics Engine** for SQL queries over stored data
- **61% cost reduction** compared to traditional database approach

**Monthly Cost: $18 vs $46 traditional = $336 annual savings**

## ðŸ—ï¸ Architecture Options

### Option 1: Google Cloud Platform (Recommended)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Run    â”‚    â”‚ Cloud Firestore â”‚    â”‚ Cloud Storageâ”‚
â”‚ (Serverless) â”‚â”€â”€â”€â”€â”‚ (Real-time)     â”‚â”€â”€â”€â”€â”‚ (Historical) â”‚
â”‚              â”‚    â”‚                 â”‚    â”‚              â”‚
â”‚ â€¢ Auto-scale â”‚    â”‚ â€¢ User data     â”‚    â”‚ â€¢ Market dataâ”‚
â”‚ â€¢ Pay per useâ”‚    â”‚ â€¢ Portfolios    â”‚    â”‚ â€¢ Backups    â”‚
â”‚ â€¢ $5/month   â”‚    â”‚ â€¢ $3/month      â”‚    â”‚ â€¢ $2/month   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  BigQuery    â”‚
                â”‚ (Analytics)  â”‚
                â”‚              â”‚
                â”‚ â€¢ SQL queriesâ”‚
                â”‚ â€¢ $3/month   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Option 2: Amazon Web Services
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS Lambda   â”‚    â”‚   DynamoDB      â”‚    â”‚      S3      â”‚
â”‚ (Serverless) â”‚â”€â”€â”€â”€â”‚ (Real-time)     â”‚â”€â”€â”€â”€â”‚ (Historical) â”‚
â”‚              â”‚    â”‚                 â”‚    â”‚              â”‚
â”‚ â€¢ Auto-scale â”‚    â”‚ â€¢ User data     â”‚    â”‚ â€¢ Market dataâ”‚
â”‚ â€¢ Pay per useâ”‚    â”‚ â€¢ Portfolios    â”‚    â”‚ â€¢ Backups    â”‚
â”‚ â€¢ $5/month   â”‚    â”‚ â€¢ $3/month      â”‚    â”‚ â€¢ $2/month   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Athena    â”‚
                â”‚ (Analytics)  â”‚
                â”‚              â”‚
                â”‚ â€¢ SQL queriesâ”‚
                â”‚ â€¢ $3/month   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Implementation Guide

### Step 1: Google Cloud Setup

#### 1.1 Enable Required Services
```bash
# Enable GCP services
gcloud services enable run.googleapis.com
gcloud services enable firestore.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable bigquery.googleapis.com

# Set up authentication
gcloud auth application-default login
```

#### 1.2 Create Storage Resources
```bash
# Create Cloud Storage bucket
gsutil mb gs://buffetbot-historical-data

# Set up BigQuery dataset
bq mk --dataset buffetbot_analytics

# Create Firestore database (done via console or gcloud)
gcloud firestore databases create --region=us-central1
```

### Step 2: Data Layer Implementation

#### 2.1 Hybrid Data Manager
```python
# buffetbot/storage/hybrid_manager.py
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from google.cloud import firestore, storage, bigquery
import logging

class HybridDataManager:
    """Manage data across Firestore, Cloud Storage, and BigQuery"""

    def __init__(self, project_id: str, bucket_name: str = "buffetbot-historical-data"):
        self.project_id = project_id
        self.bucket_name = bucket_name
        self.firestore_client = firestore.Client()
        self.storage_client = storage.Client()
        self.bigquery_client = bigquery.Client()
        self.logger = logging.getLogger(__name__)

    # Real-time operational data (Firestore)
    async def store_portfolio(self, user_id: str, portfolio_data: Dict) -> None:
        """Store portfolio data in Firestore for real-time access"""
        try:
            doc_ref = self.firestore_client.collection('portfolios').document(f"{user_id}_{portfolio_data['id']}")
            portfolio_data['last_updated'] = firestore.SERVER_TIMESTAMP
            doc_ref.set(portfolio_data, merge=True)
            self.logger.info(f"Portfolio stored for user {user_id}")
        except Exception as e:
            self.logger.error(f"Failed to store portfolio: {e}")
            raise

    async def get_user_portfolios(self, user_id: str) -> List[Dict]:
        """Get user portfolios from Firestore"""
        try:
            portfolios = []
            docs = self.firestore_client.collection('portfolios').where('user_id', '==', user_id).stream()
            for doc in docs:
                portfolio = doc.to_dict()
                portfolio['id'] = doc.id
                portfolios.append(portfolio)
            return portfolios
        except Exception as e:
            self.logger.error(f"Failed to get portfolios: {e}")
            return []

    async def store_position(self, portfolio_id: str, position_data: Dict) -> None:
        """Store position data in Firestore"""
        doc_ref = self.firestore_client.collection('positions').document(f"{portfolio_id}_{position_data['symbol']}")
        position_data['last_updated'] = firestore.SERVER_TIMESTAMP
        doc_ref.set(position_data, merge=True)

    # Historical data archival (Cloud Storage)
    async def archive_market_data(self, symbol: str, data: Dict, date: datetime = None) -> str:
        """Archive market data to Cloud Storage"""
        if date is None:
            date = datetime.now()

        try:
            bucket = self.storage_client.bucket(self.bucket_name)

            # Organize by year/month/day/symbol for efficient querying
            blob_path = f"market_data/{date.year}/{date.month:02d}/{date.day:02d}/{symbol}.json"
            blob = bucket.blob(blob_path)

            # Add metadata for easier querying
            archive_data = {
                'symbol': symbol,
                'date': date.isoformat(),
                'data': data,
                'archived_at': datetime.now().isoformat()
            }

            blob.upload_from_string(json.dumps(archive_data))
            self.logger.info(f"Archived market data for {symbol} to {blob_path}")
            return blob_path

        except Exception as e:
            self.logger.error(f"Failed to archive market data: {e}")
            raise

    async def archive_analysis_results(self, analysis_id: str, results: Dict) -> str:
        """Archive analysis results to Cloud Storage"""
        date = datetime.now()
        blob_path = f"analysis/{date.year}/{date.month:02d}/{analysis_id}.json"

        bucket = self.storage_client.bucket(self.bucket_name)
        blob = bucket.blob(blob_path)

        archive_data = {
            'analysis_id': analysis_id,
            'results': results,
            'archived_at': date.isoformat()
        }

        blob.upload_from_string(json.dumps(archive_data))
        return blob_path

    # Analytics queries (BigQuery)
    async def setup_bigquery_external_tables(self) -> None:
        """Set up BigQuery external tables pointing to Cloud Storage"""

        # Market data external table
        market_data_schema = [
            bigquery.SchemaField("symbol", "STRING"),
            bigquery.SchemaField("date", "TIMESTAMP"),
            bigquery.SchemaField("data", "JSON"),
            bigquery.SchemaField("archived_at", "TIMESTAMP")
        ]

        external_config = bigquery.ExternalConfig("CSV")
        external_config.source_uris = [f"gs://{self.bucket_name}/market_data/*/*.json"]
        external_config.schema = market_data_schema
        external_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON

        table_id = f"{self.project_id}.buffetbot_analytics.market_data"
        table = bigquery.Table(table_id, schema=market_data_schema)
        table.external_data_configuration = external_config

        table = self.bigquery_client.create_table(table, exists_ok=True)
        self.logger.info(f"Created external table: {table_id}")

    async def query_historical_data(self, query: str) -> List[Dict]:
        """Query historical data using BigQuery"""
        try:
            # Use BigQuery to query data directly from Cloud Storage
            full_query = f"""
            WITH market_data AS (
                SELECT
                    JSON_EXTRACT_SCALAR(data, '$.symbol') as symbol,
                    PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*S', JSON_EXTRACT_SCALAR(data, '$.timestamp')) as timestamp,
                    JSON_EXTRACT_SCALAR(data, '$.price') as price,
                    JSON_EXTRACT_SCALAR(data, '$.volume') as volume
                FROM `{self.project_id}.buffetbot_analytics.market_data`
            )
            {query}
            """

            query_job = self.bigquery_client.query(full_query)
            results = query_job.result()

            return [dict(row) for row in results]

        except Exception as e:
            self.logger.error(f"BigQuery query failed: {e}")
            return []

    # Data lifecycle management
    async def cleanup_old_data(self, days_to_keep: int = 365) -> None:
        """Clean up old data from Cloud Storage"""
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        bucket = self.storage_client.bucket(self.bucket_name)

        # List and delete old blobs
        blobs = bucket.list_blobs(prefix="market_data/")
        deleted_count = 0

        for blob in blobs:
            if blob.time_created.replace(tzinfo=None) < cutoff_date:
                blob.delete()
                deleted_count += 1

        self.logger.info(f"Cleaned up {deleted_count} old files")

# Usage example
async def main():
    manager = HybridDataManager("your-gcp-project-id")

    # Store real-time data
    await manager.store_portfolio("user123", {
        'id': 'portfolio1',
        'user_id': 'user123',
        'name': 'My Portfolio',
        'total_value': 50000
    })

    # Archive historical data
    market_data = {
        'symbol': 'AAPL',
        'price': 150.00,
        'volume': 1000000,
        'timestamp': datetime.now().isoformat()
    }
    await manager.archive_market_data('AAPL', market_data)

    # Query analytics
    results = await manager.query_historical_data("""
        SELECT symbol, AVG(CAST(price AS FLOAT64)) as avg_price
        FROM market_data
        WHERE timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
        GROUP BY symbol
        ORDER BY avg_price DESC
    """)

    print(f"Analytics results: {results}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Step 3: Cost Optimization Strategies

#### 3.1 Smart Data Tiering
```python
class DataTieringManager:
    """Automatically tier data based on access patterns"""

    def __init__(self, hybrid_manager: HybridDataManager):
        self.hybrid_manager = hybrid_manager

    async def tier_data_by_age(self):
        """Move old data to cheaper storage classes"""

        # Move data older than 30 days to Nearline (cheaper)
        # Move data older than 1 year to Coldline (cheapest)

        bucket = self.hybrid_manager.storage_client.bucket(self.hybrid_manager.bucket_name)

        # Set lifecycle rules
        lifecycle_rules = [
            {
                "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
                "condition": {"age": 30}
            },
            {
                "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
                "condition": {"age": 365}
            },
            {
                "action": {"type": "Delete"},
                "condition": {"age": 2555}  # 7 years
            }
        ]

        bucket.lifecycle_rules = lifecycle_rules
        bucket.patch()
```

#### 3.2 Query Cost Optimization
```python
class QueryOptimizer:
    """Optimize BigQuery costs"""

    @staticmethod
    def estimate_query_cost(query: str, bigquery_client) -> float:
        """Estimate query cost before running"""
        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
        query_job = bigquery_client.query(query, job_config=job_config)

        # BigQuery pricing: $5 per TB processed
        bytes_processed = query_job.total_bytes_processed
        tb_processed = bytes_processed / (1024**4)  # Convert to TB
        estimated_cost = tb_processed * 5  # $5 per TB

        return estimated_cost

    @staticmethod
    def add_cost_controls(query: str, max_cost: float = 1.0) -> str:
        """Add cost controls to query"""
        return f"""
        -- Maximum cost control: ${max_cost}
        SELECT * FROM (
            {query}
        )
        -- Add LIMIT to control costs
        LIMIT 10000
        """
```

### Step 4: Deployment Configuration

#### 4.1 Cloud Run Deployment
```yaml
# cloudrun.yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: buffetbot
  annotations:
    run.googleapis.com/ingress: all
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/minScale: "0"
        autoscaling.knative.dev/maxScale: "10"
        run.googleapis.com/memory: "512Mi"
        run.googleapis.com/cpu: "1"
    spec:
      containers:
      - image: gcr.io/PROJECT_ID/buffetbot:latest
        env:
        - name: STORAGE_STRATEGY
          value: "hybrid"
        - name: GCP_PROJECT_ID
          value: "your-project-id"
        - name: BUCKET_NAME
          value: "buffetbot-historical-data"
        resources:
          limits:
            memory: "512Mi"
            cpu: "1"
```

#### 4.2 Environment Configuration
```python
# config/hybrid_settings.py
import os
from dataclasses import dataclass

@dataclass
class HybridStorageConfig:
    """Configuration for hybrid storage strategy"""

    # GCP Settings
    gcp_project_id: str = os.getenv('GCP_PROJECT_ID')
    bucket_name: str = os.getenv('BUCKET_NAME', 'buffetbot-historical-data')
    bigquery_dataset: str = 'buffetbot_analytics'

    # Data retention policies
    hot_data_days: int = 30      # Keep in Firestore
    warm_data_days: int = 365    # Keep in Standard storage
    cold_data_years: int = 7     # Keep in Coldline

    # Cost controls
    max_query_cost: float = 1.0   # Max $1 per query
    daily_query_budget: float = 10.0  # Max $10/day on queries

    # Performance settings
    batch_size: int = 1000
    cache_ttl: int = 3600  # 1 hour

    def get_storage_class_by_age(self, days_old: int) -> str:
        """Determine storage class based on data age"""
        if days_old <= self.hot_data_days:
            return "STANDARD"
        elif days_old <= self.warm_data_days:
            return "NEARLINE"
        else:
            return "COLDLINE"
```

## ðŸ“Š Cost Monitoring & Optimization

### Cost Tracking Script
```python
# scripts/hybrid_cost_monitor.py
from google.cloud import billing
from google.cloud import bigquery
import json
from datetime import datetime, timedelta

class HybridCostMonitor:
    """Monitor costs for hybrid storage strategy"""

    def __init__(self, project_id: str, billing_account_id: str):
        self.project_id = project_id
        self.billing_account_id = billing_account_id
        self.bigquery_client = bigquery.Client()

    def get_daily_costs(self) -> dict:
        """Get daily costs broken down by service"""

        # Query billing data
        query = f"""
        SELECT
            service.description as service_name,
            SUM(cost) as total_cost,
            currency
        FROM `{self.project_id}.cloud_billing_export.gcp_billing_export_v1_{self.billing_account_id}`
        WHERE DATE(usage_start_time) = CURRENT_DATE() - 1
        GROUP BY service.description, currency
        ORDER BY total_cost DESC
        """

        query_job = self.bigquery_client.query(query)
        results = query_job.result()

        costs = {}
        for row in results:
            costs[row.service_name] = {
                'cost': float(row.total_cost),
                'currency': row.currency
            }

        return costs

    def generate_cost_report(self) -> dict:
        """Generate comprehensive cost report"""
        costs = self.get_daily_costs()

        # Map services to our categories
        service_mapping = {
            'Cloud Run': 'hosting',
            'Cloud Firestore': 'database',
            'Cloud Storage': 'storage',
            'BigQuery': 'analytics'
        }

        categorized_costs = {}
        total_cost = 0

        for service, cost_info in costs.items():
            category = service_mapping.get(service, 'other')
            categorized_costs[category] = categorized_costs.get(category, 0) + cost_info['cost']
            total_cost += cost_info['cost']

        return {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'total_cost': round(total_cost, 2),
            'breakdown': categorized_costs,
            'projected_monthly': round(total_cost * 30, 2)
        }

# Usage
monitor = HybridCostMonitor("your-project-id", "your-billing-account-id")
report = monitor.generate_cost_report()
print(json.dumps(report, indent=2))
```

## ðŸŽ¯ Migration Strategy

### From Traditional Database to Hybrid

#### Phase 1: Dual Write (2 weeks)
```python
class MigrationManager:
    """Manage migration from traditional DB to hybrid storage"""

    def __init__(self, traditional_db, hybrid_manager):
        self.traditional_db = traditional_db
        self.hybrid_manager = hybrid_manager

    async def dual_write_portfolio(self, portfolio_data):
        """Write to both traditional DB and new hybrid storage"""
        try:
            # Write to traditional database
            await self.traditional_db.store_portfolio(portfolio_data)

            # Write to new hybrid storage
            await self.hybrid_manager.store_portfolio(
                portfolio_data['user_id'],
                portfolio_data
            )

        except Exception as e:
            # Rollback if either fails
            await self._rollback_portfolio(portfolio_data)
            raise
```

#### Phase 2: Read Migration (1 week)
```python
async def migrated_read_portfolio(self, user_id: str):
    """Read from hybrid storage with fallback to traditional"""
    try:
        # Try new storage first
        portfolios = await self.hybrid_manager.get_user_portfolios(user_id)
        if portfolios:
            return portfolios
    except Exception:
        pass

    # Fallback to traditional database
    return await self.traditional_db.get_user_portfolios(user_id)
```

#### Phase 3: Historical Data Migration (2 weeks)
```python
async def migrate_historical_data(self):
    """Migrate historical data to Cloud Storage"""

    # Get all historical market data
    historical_data = await self.traditional_db.get_all_market_data()

    for data_chunk in self._chunk_data(historical_data, 1000):
        for record in data_chunk:
            await self.hybrid_manager.archive_market_data(
                record['symbol'],
                record['data'],
                record['date']
            )

        # Pause between chunks to avoid rate limits
        await asyncio.sleep(1)
```

## âœ… Benefits Summary

### Cost Benefits
- **61% cost reduction**: $46/month â†’ $18/month
- **Scalable pricing**: Pay only for what you use
- **No over-provisioning**: Serverless auto-scaling

### Performance Benefits
- **Real-time data**: Sub-100ms response times with Firestore
- **Unlimited storage**: Petabyte-scale Cloud Storage
- **Powerful analytics**: BigQuery for complex queries

### Operational Benefits
- **No maintenance**: Fully managed services
- **Auto-scaling**: Handle traffic spikes automatically
- **High availability**: 99.95% uptime SLA

### Implementation Timeline
- **Week 1-2**: Set up GCP resources and hybrid manager
- **Week 3-4**: Implement dual-write migration
- **Week 5**: Switch reads to hybrid storage
- **Week 6-7**: Migrate historical data
- **Week 8**: Decommission traditional database

**Result**: $336 annual savings with better performance and scalability! ðŸš€

---

*Your friend was absolutely right - this hybrid approach is a game-changer for cost optimization while maintaining enterprise-grade functionality.*
